#  Reinforcement Learning (RL)

- **马尔科夫决策化过程：描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题**

- **马尔科夫性：某时刻状态只取决于上一时刻状态**

- **贝尔曼方程推导：**

  $R_t:t \ 时刻下的即时奖励$
  
  $\gamma:折扣因子(\gamma \in [0,1])$
  
  - $为避免出现状态循环的情况$
  
  - $系统对未来的预测不一定准确，所以需要打一定折扣$
  - $折扣因子越趋近于1，考虑的利益越长远$
  
  $G_t:时间从\ t\ 到结束的累计奖赏 \ G_t = R_{t+1}+ \gamma \ R_{t+2}+ \gamma^2 \ R_{t+3}......$
  
  $V_{\pi}(s):策略为\ \pi \ ，状态s下预计累计回报期望值 \ V_{\pi}(s)=E[G_{t}|S_t=s]$
  $$
  \begin{align*}
  V_{\pi}(s) &= E[G_t|S_t=s] \\
  &= E[R_{t+1}+\gamma \ E[G_{t+1}|S_{t+1}=s']\ | \ S_t=s] \\
  &= E[R_{t+1}+\gamma \ V_{\pi}(s_{t+1})|S_t=s]
  \end{align*}
  $$

# Q learning

**核心思想：**通过学习一个动作价值函数（Q函数）来评估在某个状态下采取某个动作的期望作用。

#### 一、基本概念

1. 状态（State）：环境的某个特定状态或配置
2. 动作（Action）：在当前状态下可以采取的动作行为
3. 奖励（Reward）：采取某个动作到达另一状态后从环境中获得的即时回报
4. 策略（Policy）：从状态到动作的映射，指导如何根据当前状态选择动作。
5. Q函数（Q-value）：表示在状态s下采取动作a的期望回报。

#### 二、Q函数定义

​    $Q$ 函数 $Q(s,a)$ 定义为从状态 $s$ 采取动作 $a$ 并遵循选择最优回报策略后获得的期望回报，数学表达式为：
$$
Q(s,a)=E[R_{t+1}+\gamma max_{a'}Q(s_{t+1},a')|s_t=s,a_t=a]
$$

其中：

- $R_{t+1}$是在时间 $t+1$ 获得的奖励

- $\gamma$ 是折扣因子，决定了未来奖励在当前的价值

- $s_{t+1}$ 是在时间 $t+1$ 的新状态

- $a’$ 是新状态下可能采取的动作

#### 三、Q 学习算法步骤

1. 初始化：为每个状态-动作对 $(s，a)$ 初始化 $Q$ 值，通常设为0或小的随机数

2. 探索和利用：在每个时间步 $t$ ，根据当前的 $Q$ 值选择一个动作。这可以通过 $\epsilon-$ 贪婪策略来实现，即以 $1-\epsilon$ 的概率选择当前 $Q$ 值最高的动作，以 $\epsilon$ 的概率随机选择一个动作

3. 执行动作：执行选定的动作，并观察新的状态 $s_{t+1}$ 和获得的奖励 $R_{t+1}$

4. 更新$Q$值：根据观察到的奖励和新状态，更新当前状态-动作对的$Q$值 ，更新公式为：
$$
   Q(s_t,a_t)=Q(s_t,a_t)+\alpha[R_{t+1}+\gamma \ max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)]
$$

5. 重复操作：重复 2 - 4 步，直到满足中止条件，如达到最大迭代次数或$Q$值收敛

# DQN

当状态空间连续，环境状态无穷多的情况下无法构建q表存储价值并逐步更新训练智能体，因此加入神经网络来拟合一个函数$Q(s,a,w)$来近似动作价值$Q(s，a)$，$w$为神经网络参数。

#### 一、目标网络（Target Network）

估计函数Q的神经网络（Estimation Network）在训练时，权重改变，动作价值的估计也随之改变。学习的过程中，动作价值试图追逐一个变化的回报，不稳定性造成拟合困难。因此新创建另一个神经网络作为目标网络，其参数在每训练一定次数之后依据估计网络改变即一段时间内固定不变，使得拟合过程相对容易。两个神经网络结构相同，作用不同；估计网络用于控制智能体收集经验，计算估计价值，目标网络用于接受经验，计算目标价值。


#### 二 、经验回放（Experience Replay）

一方面为重复使用经验提高数据利用率，另一方面减少相邻时序变化相关性对神经网络训练的影响，采用经验回放措施。

- 创建记忆库$D$存入每次transition获得的数据$<s_t,a_t,r_t,s_{t+1}>$，超出记忆库存储最大值时去除部分数据
- 每次从记忆库$D$中取出一个数据$(s_t,a_t,r_t,s_{t+1})\in D$
- 对于取出的样本数据计算目标值$Target: r + \gamma \ max_{a'}Q(s',a',w)$
- 通过优化器和损失函数反向传播更新网络权重

#### 三、流程图：
![dqn算法流程图](D:\my project\AI\二轮\RL\img\dqn.png)

