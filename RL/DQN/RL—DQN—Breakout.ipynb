{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置每损失一条命结束一回合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info, _ = self.env.step(action)\n",
    "        self.real_done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info, _\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        if self.real_done:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs, _, _, _, info = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 280\n",
    "env = EpisodicLifeEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超参数定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练每批样本数\n",
    "batch_size = 32\n",
    "# 动作维度\n",
    "action_size = env.action_space.n\n",
    "# 状态维度\n",
    "observation_space = env.observation_space\n",
    "# 学习率\n",
    "learning_rate = 1e-4\n",
    "# reward discount\n",
    "gamma = 0.99\n",
    "# greedy policy\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 1000000\n",
    "epsilon_random_count = 5000\n",
    "# 目标网络更新频率\n",
    "target_update = 1000\n",
    "# sample capacity\n",
    "memory_capacity = 100000\n",
    "# 记忆库样本形式\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"new_state\", \"reward\"))\n",
    "# 设备选择\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**经验回放，存入达到batch_size的数据后，每次随机提取batch_size大小批次的数据对估计网络进行训练，既避免了相邻动作状态相关性大训练不准确的问题，又能保证过往产生的经验数据充分利用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经验回放\n",
    "class Replay_Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # 向记忆库中存入样本数据\n",
    "    def push(self, args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积神经网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, action_size):\n",
    "        super(CNN, self).__init__()\n",
    "        # 特征提取\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # 不同动作价值计算\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size),\n",
    "        )\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classify(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN动作选择及模型训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self, in_channels, action_space, gamma, learning_rate, memory_capacity\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.action_space = action_space\n",
    "        self.action_size = action_space.n\n",
    "        self.stepdone = 0\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate\n",
    "        # 记忆库\n",
    "        self.memory_buffer = Replay_Memory(memory_capacity)\n",
    "        # 价值估计网络\n",
    "        self.estimate_cnn = CNN(in_channels, action_size)\n",
    "        # 目标网络\n",
    "        self.target_cnn = CNN(in_channels, action_size)\n",
    "        # 优化器\n",
    "        self.optimizer = optim.Adam(self.estimate_cnn.parameters(), lr=self.lr)\n",
    "        # 损失函数\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.stepdone += 1\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(\n",
    "            -(self.stepdone / epsilon_decay)\n",
    "        )\n",
    "        rand = np.random.uniform(0, 1)\n",
    "        print(self.stepdone)\n",
    "        if self.stepdone < epsilon_random_count or rand < epsilon:\n",
    "            action = torch.tensor(\n",
    "                [[np.random.choice(self.action_size)]], dtype=torch.long\n",
    "            )\n",
    "        else:\n",
    "            action = self.estimate_cnn(state).detach().max(1)[1].view(1, 1)\n",
    "        return action\n",
    "\n",
    "    def learn(self, args):\n",
    "        # 存入记忆库\n",
    "        self.memory_buffer.push(args)\n",
    "        # 记忆库中样本数量小于batch_size则跳过学习\n",
    "        if self.memory_buffer.__len__() < batch_size:\n",
    "            return\n",
    "        # 取样\n",
    "        transitions = self.memory_buffer.sample(batch_size)\n",
    "        # 分类解压\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # 转化数据为元组\n",
    "        actions = batch.action\n",
    "        rewards = tuple(map(lambda r: torch.tensor([r]), batch.reward))\n",
    "        # 下一状态为非终止状态的掩模 [32]\n",
    "        no_terminal_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.new_state)), dtype=torch.uint8\n",
    "        ).bool()\n",
    "        # 当前批次样本数据\n",
    "        # [?,4,84,84]\n",
    "        batch_new_state = torch.cat([s for s in batch.new_state if s is not None])\n",
    "        # [32,4,84,84]\n",
    "        batch_state = torch.cat(batch.state)\n",
    "        # [32,1]\n",
    "        batch_action = torch.cat(actions)\n",
    "        # [32]\n",
    "        batch_reward = torch.cat(rewards)\n",
    "        # 估计Q值(采取batch_action行动的预测Q值) [32,1]\n",
    "        estimate_Q = self.estimate_cnn(batch_state).gather(1, batch_action).view(32)\n",
    "        # 目标Q值(采取batch_action后达到新状态的目标Q值) [32]\n",
    "        target_Q = torch.zeros(batch_size)\n",
    "        target_Q[no_terminal_mask] = (\n",
    "            self.gamma * self.target_cnn(batch_new_state).max(1)[0]\n",
    "            + batch_reward[no_terminal_mask]\n",
    "        )\n",
    "        # 反向传播\n",
    "        loss = self.criterion(estimate_Q, target_Q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.estimate_cnn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "dqn = DQN(4, env.action_space, gamma, learning_rate, memory_capacity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**状态预处理为作为神经网络输入的形式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_process(observation):\n",
    "    # 裁剪、转化为灰度图\n",
    "    img = cv.cvtColor(cv.resize(observation, (84, 84)), cv.COLOR_BGR2GRAY)\n",
    "    # 二值化阈值处理\n",
    "    ret, img = cv.threshold(img, 1, 255, cv.THRESH_BINARY)\n",
    "    # 堆叠四帧为一个状态\n",
    "    state = np.stack((img, img, img, img), axis=0)\n",
    "    return np.reshape(state, (1, 4, 84, 84))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 迭代训练次数\n",
    "episodes = 10000\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(i)\n",
    "    # 初始化\n",
    "    observation = env.reset()[0]\n",
    "    # 渲染\n",
    "    env.render()\n",
    "    while True:\n",
    "        # 渲染\n",
    "        env.render()\n",
    "        # 状态处理\n",
    "        state = pre_process(observation)\n",
    "        # 动作选择\n",
    "        action = dqn.choose_action(state)\n",
    "        # 环境转换\n",
    "        observation_, reward, done, info, _ = env.step(action)\n",
    "        # 新状态处理\n",
    "        state_ = pre_process(observation_)\n",
    "        # 训练学习\n",
    "        values = (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            action,\n",
    "            torch.tensor(state_, dtype=torch.float32),\n",
    "            reward,\n",
    "        )\n",
    "        dqn.learn(values)\n",
    "        # 状态转换\n",
    "        state = state_\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        dqn.target_cnn = dqn.estimate_cnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看训练结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 280\n",
    "# 初始化\n",
    "observation = env.reset()[0]\n",
    "# 渲染\n",
    "env.render()\n",
    "while True:\n",
    "    # 渲染\n",
    "    env.render()\n",
    "    # 状态处理\n",
    "    state = pre_process(observation)\n",
    "    # 动作选择\n",
    "    action = dqn.choose_action(state)\n",
    "    # 环境转换\n",
    "    observation_, reward, done, info, _ = env.step(action)\n",
    "    # 状态转换\n",
    "    state = state_\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
